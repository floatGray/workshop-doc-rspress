---
pageType: doc
---

import MemberPanel from '../components/MemberPanel';
import Member from '../components/Member';
import PersonImage from '../components/PersonImage';

# Vision-Language Models for Navigation and Manipulation (VLMNM)

#### Full-day hybrid workshop at [ICRA 2024](https://2024.ieee-icra.org/)

#### Friday, May 17, 2024, [Japan Standard Time (JST)](https://greenwichmeantime.com/time/japan/), [Yokohama (Japan)](https://2024.ieee-icra.org/yokohama/conference-venue/)

## Introduction

With the rising capabilities of LLMs and VLMs, the past two years have seen a surge in research work using VLMs for navigation and manipulation. Fusing the capabilities of visual interpretation with natural language processing, these models are poised to redefine how robotic systems interact with both their environment and human counterparts. The relevance of this topic cannot be overstated; as the frontier of human-robot interaction expands, so does the necessity for robots to comprehend and operate within complex environments using naturalistic instructions. Our workshop will not only reflect the state-of-the-art advancements in this domain, by featuring a diverse set of speakers, from senior academics to researchers in early careers, from industry researchers to companies producing mobile manipulation platforms, from researchers who are enthusiastic about using VLMs for robotics to those who have reservations about it. We aim for this event to be a catalyst for originality and diversity at ICRA 2024. We believe that, amidst a sea of workshops, ours will provide unique perspectives that will push the boundaries of what's achievable in robot navigation and manipulation.

In this workshop, we plan to discuss:

- How can VLMs/LLMs enhance robotics navigation and manipulation?
- How to extract world knowledge from pre-trained VLMs/LLMs and apply them to navigation and manipulation?
- How to integrate VLMs/LLMs with robot components, such as perception, control, and planning? How to account for partial observability and uncertainty?
- Benchmarks and datasets to assess the generalization capabilities of VLMs/LLMs for navigation and manipulation.
- Capabilities and limitations of VLMs/LLMs for navigation and manipulation (e.g. in task planning, spatial understanding)
- New interaction modes between robots and humans enabled by VLMs/LLMs.

## Call for Papers

We invite submissions including but not limited to the following topics:

- Applications:
  - Integration of VLM/LLMs for manipulation and navigation
  - VLM/LLMs for perception/scene understanding/state estimation
  - VLM/LLMs for control/skill learning/motion generation
  - VLM/LLMs for decision-making/reasoning/planning
  - VLM/LLMs as world models
  - VLMs/LLMs for multimodal task specifications
  - VLMs/LLMs for human-robot/robot-robot interactions
  - VLMs/LLMs for scene and task generation
- New Capabilities:
  - Open-vocabulary perception/navigation/manipulation
  - Commonsense reasoning with VLM/LLMs
  - Generalization to unseen object categories, environments, and tasks
  - Bootstrapping learning from scarce data
  - Natural language interaction with everyday users
- Datasets/Benchmarks:
  - Internet-scale data for training robotics foundation models
  - Mobile manipulation benchmarks for VLM/LLM-based systems
- Limitations:
  - Failure modes of VLM/LLMs
  - Robustness of VLM/LLMs
  - Certifiabilities of VLM/LLMs

Submissions should have up to 4 pages of technical content, with no limit on references/appendices. Submissions are suggested to follow the ICRA double-column format with the template available **[here](https://ras.papercept.net/conferences/support/support.php)**. We encourage authors to upload videos, code, or data as supplementary material (due on the same day as the paper). Following the main conference, our workshop will use a **single-blind** review process. We welcome both unpublished, original contributions and recently published relevant works. Accepted papers will be presented as posters or orals and made public via the workshop’s OpenReview page with the authors’ consent. We strongly encourage at least one of the authors to **present on-site** during the workshop. Our workshop will feature a Best Paper Award.

Important Dates:

- Submission portal opens: January 29, 2024
- Paper submission deadline: **March 11, Monday, 2024 (AoE)**
- Notification of acceptance: March 27, 2024 March 29, 2024
- Camera-Ready deadline: April 26, 2024
- Workshop @ ICRA 2024: May 13 or 17, 2024 (exact date will be updated when the conference finalizes the schedule)

Submissions will be accepted through **[OpenReview](https://openreview.net/group?id=IEEE.org/2024/ICRA/Workshop/VLMNM)**. Submissions will not be public during the review process. Only accepted papers will be made public.

## Tentative Schedule

| Time ([JST](https://greenwichmeantime.com/time/japan/)) | Event                                                                                                                                                                                | Description                                                                                       | Time ([PDT](https://greenwichmeantime.com/time-zone/usa/pacific-daylight-time/)) (1 Day Earlier) |
| :-----------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------ | :----------------------------------------------------------------------------------------------: |
|                       8:30 - 8:50                       | Coffee and Pasteries                                                                                                                                                                 |                                                                                                   |                                          15:30 - 15:50                                           |
|                       8:50 - 9:00                       | Introduction                                                                                                                                                                         | **by the Organizing Committee**                                                                   |                                          15:50 - 16:00                                           |
|                       9:00 - 9:20                       | LLM-State: Adaptive State Representation for Long-Horizon Task Planning in the Open World [Prof. David Hsu](https://www.comp.nus.edu.sg/~dyhsu/) \| National University of Singapore | <PersonImage imgUrl="https://vlmnm-workshop.github.io/photos/Speakers/DavidHsu.jpeg" />           |                                          16:00 - 16:20                                           |
|                       9:20 - 9:40                       | Limitations of using LLM for Planning [Prof. Subbarao Kambhampati](https://search.asu.edu/profile/95646) \| Arizona State University                                                 | <PersonImage imgUrl="https://vlmnm-workshop.github.io/photos/Speakers/SubbaraoKambhampati.png" /> |                                          16:20 - 16:40                                           |
|                      9:40 - 10:00                       | LLM-based Task and Motion Planning for Robots [Prof. Chuchu Fan](https://chuchu.mit.edu/) \| Massachusetts Institute of Technology                                                   | <PersonImage imgUrl="https://vlmnm-workshop.github.io/photos/Speakers/ChuchuFan.jpeg" />          |                                          16:40 - 17:00                                           |

## Organizers

<MemberPanel>
  <Member
    name="Chris Paxton"
    organization="FAIR, Meta"
    imgUrl="https://vlmnm-workshop.github.io/photos/Organizers/ChrisPaxton.jpeg"
  />
  <Member
    name="Chris Paxton"
    organization="FAIR, Meta"
    imgUrl="https://vlmnm-workshop.github.io/photos/Organizers/ChrisPaxton.jpeg"
  />
  <Member
    name="Chris Paxton"
    organization="FAIR, Meta"
    imgUrl="https://vlmnm-workshop.github.io/photos/Organizers/ChrisPaxton.jpeg"
  />
  <Member
    name="Chris Paxton"
    organization="FAIR, Meta"
    imgUrl="https://vlmnm-workshop.github.io/photos/Organizers/ChrisPaxton.jpeg"
  />
  <Member
    name="Chris Paxton"
    organization="FAIR, Meta"
    imgUrl="https://vlmnm-workshop.github.io/photos/Organizers/ChrisPaxton.jpeg"
  />
  <Member
    name="Chris Paxton"
    organization="FAIR, Meta"
    imgUrl="https://vlmnm-workshop.github.io/photos/Organizers/ChrisPaxton.jpeg"
  />
</MemberPanel>

## Contact

For further information or questions, please contact us at vlm-navigation-manipulation-workshop [AT] googlegroups [DOT] com
